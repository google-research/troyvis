# simple training setting (8GPUs, 10k iter)
_BASE_: "base_clip_teacher_image_R50.yaml"
FIND_UNUSED_PARAMETERS: False
MODEL:
  WEIGHTS: "exp/GLEE_Lite_CLIPfrozen_pretrain_baseline_new_16g_evaclipl/model_final.pth"
  VISUAL_PROMPT: False
  EARLYFUSION: True
  EFFI_EARLY_FUSION: True
  EFFI_LVL: 3
  LANGUAGE_BACKBONE:
    LANG_DIM: 768
  TEXT:
    ARCH: EVA02_L_CLIP_teacher
DATASETS:
  TRAIN: ("openimage_train", "objects365_v2_train", "lvis_v1_train", "vg_train", "vg_captiontrain", "coco_2017_train", "image_yt19", "image_yt21", "image_o",)
  TEST: ("coco_2017_val", "lvis_v1_minival",)  
SOLVER:  
  IMS_PER_BATCH: 32
  BASE_LR: 0.0001
  STEPS: (60000,80000 )
  MAX_ITER: 100000
  CHECKPOINT_PERIOD: 10000
  TEXTENCODER_MULTIPLIER: 0.1
TEST:
  EVAL_PERIOD: 0
DATALOADER:
  SAMPLER_TRAIN: "MultiDatasetSampler"
  DATASET_RATIO: [3, 3, 1.5, 1.5, 1.5, 1.5, 0.3, 0.3, 0.3]
  USE_DIFF_BS_SIZE: True
  DATASET_BS: [2, 2, 2, 2, 2, 2, 2, 2, 2]
  USE_RFS: [True, True, True, False, False, False, False, False, False]
  DATASET_ANN: ['box', 'box', 'box', 'box', 'box', 'box', 'box', 'box', 'box']
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 2
OUTPUT_DIR: ./exp/GLEE_Lite_CLIPteacher_joint_baseline_new_16g_evaclipl